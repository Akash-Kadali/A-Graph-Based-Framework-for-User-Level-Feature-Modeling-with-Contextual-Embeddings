{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"94816e46c11e4f10bde632542a5e363a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3aeae68f92d410eb33bcd9a63589e08","IPY_MODEL_a627c7ef62e8406786837edf4100a959","IPY_MODEL_97ce9945b5c6433ba11384a105c6f952"],"layout":"IPY_MODEL_7482c474e3e34fe3b24e4affd2620b57"}},"f3aeae68f92d410eb33bcd9a63589e08":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c365fa89dba41a790d99f6f3d184cf0","placeholder":"​","style":"IPY_MODEL_fd230d197d1e48da9a9ede8dd27d2c07","value":"Map: 100%"}},"a627c7ef62e8406786837edf4100a959":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaa2d552e3384bfea30900b9e43431c3","max":5860313,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69614ae3c3984ec9a1f418fbe00e792a","value":5860313}},"97ce9945b5c6433ba11384a105c6f952":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36559cbbd0694743bd6033160f454f2c","placeholder":"​","style":"IPY_MODEL_8c327ba430de4aceb23d047ee9844d53","value":" 5860313/5860313 [09:16&lt;00:00, 12839.81 examples/s]"}},"7482c474e3e34fe3b24e4affd2620b57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c365fa89dba41a790d99f6f3d184cf0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd230d197d1e48da9a9ede8dd27d2c07":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aaa2d552e3384bfea30900b9e43431c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69614ae3c3984ec9a1f418fbe00e792a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"36559cbbd0694743bd6033160f454f2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c327ba430de4aceb23d047ee9844d53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72f1d88d1de3451bb5b6b3ed55f30a6a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b8e243e3daa4721bdff3b346543598f","IPY_MODEL_5726d1fa052d4a75bed925a8a308a547","IPY_MODEL_ede09ab7c2074ec4b2c745a1ef7a4ad7"],"layout":"IPY_MODEL_d47b2379828d49bba864d5d2238bc0a0"}},"3b8e243e3daa4721bdff3b346543598f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40f1b2104d9d4f369a3955d83e53a0a6","placeholder":"​","style":"IPY_MODEL_2440d994af3c4bab8241c46d407a58fd","value":"Map: 100%"}},"5726d1fa052d4a75bed925a8a308a547":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d2f650f865845e4aa2dc08fe01448be","max":1306221,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad1291597a7548838e13af625ee376a6","value":1306221}},"ede09ab7c2074ec4b2c745a1ef7a4ad7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf45777b34284f10839502187464eb31","placeholder":"​","style":"IPY_MODEL_156089806d6f4d6fb5b5caf9ab163dd9","value":" 1306221/1306221 [02:14&lt;00:00, 7500.35 examples/s]"}},"d47b2379828d49bba864d5d2238bc0a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40f1b2104d9d4f369a3955d83e53a0a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2440d994af3c4bab8241c46d407a58fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d2f650f865845e4aa2dc08fe01448be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad1291597a7548838e13af625ee376a6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bf45777b34284f10839502187464eb31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"156089806d6f4d6fb5b5caf9ab163dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8283683,"sourceType":"datasetVersion","datasetId":4919914},{"sourceId":8294030,"sourceType":"datasetVersion","datasetId":4927283},{"sourceId":8297174,"sourceType":"datasetVersion","datasetId":4928773}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install datasets\n!pip install transformers[torch]\n!pip install accelerate -U\n!rm -rf /kaggle/working/*\nfrom abc import ABC, abstractmethod\nimport pandas as pd\nimport numpy as np\n\nclass Model(ABC):\n    \"\"\"\n    Abstract class for a machine learning model. Whenever it is needed to\n    implement a new model it should inherit and implement each of its methods.\n    Each inheritted model might be implemented differently but should respect\n    the signature of the abstract class.\n    \"\"\"\n\n    def __init__(self, output_dir: str) -> None:\n        self.output_dir = output_dir\n\n    @abstractmethod\n    def fit(self,\n            x_train: pd.Series,\n            y_train: pd.Series,\n            x_dev: pd.Series = None,\n            y_dev: pd.Series = None):\n        \"\"\"\n        Abstract fit method that takes training text documents `x_train` and\n        their labels `y_train` and train a model. `x_dev` and `y_dev` can be\n        used to obtain cross-validation insights, early stopping, or simply\n        ignore them.\n\n        parameters:\n            - `x_train` (pd.Series[str]) training text documents.\n            - `y_train` (pd.Series[int]) training labels.\n            - `x_dev` (pd.Series[str]) dev text documents.\n            - `y_dev` (pd.Series[int]) dev labels.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def predict(self, x: pd.Series) -> np.array:\n        \"\"\"\n        Abstract method to perform classification on samples in `x`.\n\n        parameters:\n            - `x` (pd.Series[str]) sample to predict.\n\n        returns:\n            - `y_pred` (np.array[int]) class labels for sample `x`.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def predict_proba(self, x: pd.Series) -> np.array:\n        \"\"\"\n        Abstract method to estimate classification probabilities on samples in\n        `x`.\n\n        parameters:\n            - `x` (pd.Series[str]) sample to predict.\n\n        returns:\n            - `y_pred` (np.array of floats with n classes columns) probability\n              labels for sample `x`.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def save_model(self) -> None:\n        \"\"\"\n        Save model weights as a pickle python file in `self.output_dir` using\n        its identifier `self.model_name`.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load_model(self, model_dirpath: str) -> None:\n        \"\"\"\n        Load model weights. It takes directory path `model_dirpath` where the\n        model necessary data is in.\n\n        parameters:\n            - `model_dirpath` (str) Directory path where the model is saved.\n        \"\"\"\n        pass","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1db459dd","outputId":"8a5b085d-ebd2-42d1-9ff4-e4d5efeabef6","execution":{"iopub.status.busy":"2024-05-03T03:29:16.073238Z","iopub.execute_input":"2024-05-03T03:29:16.073949Z","iopub.status.idle":"2024-05-03T03:30:07.535499Z","shell.execute_reply.started":"2024-05-03T03:29:16.073914Z","shell.execute_reply":"2024-05-03T03:30:07.534415Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.29.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n                          TextClassificationPipeline, TrainingArguments,\n                          Trainer, DataCollatorWithPadding, set_seed)\nfrom datasets import Dataset\nimport pandas as pd\nimport numpy as np\nimport os\nimport warnings\n\n\nclass TransformerModel(Model):\n    \"\"\"\n    Huggingface Transformer model for classification such as BERT, DeBERTa,\n    RoBERTa, etc.\n\n    parameters:\n        - `output_dir` (str) Directory path where the model outputs will be\n          recorded. That is weights, predictions, etc.\n\n        - `model_name` (str) Identifier of the model. It is used to recognize an\n          instance of the class. For example, if multiple runs are executed with\n          different parameters, `model_name` can be used to assign a different\n          name. Also, when saving an instance of the model, it will create a\n          directory using this parameters as its name and will be saved in\n          `output_dir`.\n\n        - `huggingface-path` (str) the name of the model in the hub of\n          huggingface. For example: `bert-base-uncased` or\n          `microsoft/deberta-v3-large`.\n\n        - `checkpoint-path` (str) [optional] path to a huggingface checkpoint\n        directory containing its configuration.\n\n        - `epochs` (int) number of epochs for training the transformer.\n\n        - `batch-size` (int) batch size used for training the transformer.\n\n        - `random_state` (int) integer number to initialize the random state\n          during the training process.\n\n        - `lr` (float) learning rate for training the transformer.\n\n        - `weight-decay` (float) weight decay penalty applied to the\n          transformer.\n\n        - `device` (str) Use `cpu` or `gpu`.\n    \"\"\"\n\n    def __init__(self,\n                 huggingface_path: str = \"bert-base-uncased\",\n                 checkpoint_path: str = None,\n                 epochs: int = 4,\n                 batch_size: int = 32,\n                 random_state: int = 42,\n                 lr: float = 2e-5,\n                 weight_decay: float = 0.01,\n                 num_labels: int = 2,\n                 output_dir: str = \"./default_output_dir\",\n                 device: str = \"cpu\") -> None:\n        super(TransformerModel, self).__init__(output_dir)\n\n        set_seed(random_state)\n\n        # Load model from hugginface hub.\n        model = AutoModelForSequenceClassification.from_pretrained(\n            huggingface_path,\n            num_labels=num_labels,\n            output_attentions=False,\n            output_hidden_states=False,\n        )\n\n        # Load tokenizer from huggingface hub.\n        tokenizer = AutoTokenizer.from_pretrained(huggingface_path,\n                                                  do_lower_case=True)\n        # Set class attributes.\n        self.model = model\n        self.tokenizer = tokenizer\n        self.checkpoint_path = checkpoint_path\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.random_state = random_state\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.device = device\n        self.num_labels = num_labels\n        self.args = None\n        self.trainer = None\n\n    def set_training_args(self):\n        self.args = TrainingArguments(\n            output_dir=self.output_dir,\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            logging_strategy=\"epoch\",\n            learning_rate=self.lr,\n            per_device_train_batch_size=self.batch_size,\n            per_device_eval_batch_size=self.batch_size,\n            num_train_epochs=self.epochs,\n            weight_decay=self.weight_decay,\n            seed=self.random_state,\n            #device=self.device,\n            #data_seed=self.random_state,\n            optim=\"adamw_hf\")\n\n    def tokenize(self, example: str):\n        \"\"\"\n        Tokenize a sentence using the model tokenizer.\n        \"\"\"\n        return self.tokenizer(example[\"text\"], truncation=True)\n\n    def build_loader(self, sentences: pd.Series, labels: pd.Series = None):\n        \"\"\"\n        Create a Dataset loader from huggingface tokenizing each sentence.\n\n        parameters:\n            - `sentences` (pd.Series[str])\n            - `labels` (pd.Series[int])\n        \"\"\"\n        dataset = Dataset.from_dict({\"text\": sentences}\n                                    | ({\n                                        \"label\": labels\n                                    } if labels is not None else {}))\n        return dataset.map(self.tokenize, batched=True)\n\n    def fit(self,\n            x_train: pd.Series,\n            y_train: pd.Series,\n            x_dev: pd.Series = None,\n            y_dev: pd.Series = None) -> None:\n        \"\"\"\n        Fit method that takes training text documents `x_train` and their labels\n        `y_train` and train a transformer based model. In this case the `x_dev`\n        and `y_dev` are used to evaluate the model in each epoch. When saving\n        the model, train and dev losses are saved too.\n\n        parameters:\n            - `x_train` (pd.Series[str]) training text documents.\n            - `y_train` (pd.Series[int]) training labels.\n            - `x_dev` (pd.Series[str]) dev text documents.\n            - `y_dev` (pd.Series[int]) dev labels.\n        \"\"\"\n        self.set_training_args()\n\n        # Create data collator.\n        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer,\n                                                padding=True)\n\n        # Create dataset loaders for train and dev sets.\n        train = self.build_loader(sentences=x_train, labels=y_train)\n        dev = self.build_loader(sentences=x_dev, labels=y_dev)\n\n        # Move huggingface model to the device indicated.\n        self.model = self.model.to(self.device)\n\n        # Instance huggingface Trainer.\n        self.trainer = Trainer(model=self.model,\n                               args=self.args,\n                               train_dataset=train,\n                               eval_dataset=dev,\n                               tokenizer=self.tokenizer,\n                               data_collator=data_collator)\n\n        # If there is any checkpoint provided, training is resumed from it.\n        if self.checkpoint_path is not None:\n            self.trainer.train(self.checkpoint_path)\n        else:\n            self.trainer.train()\n\n    def predict_proba(self, x: pd.Series) -> np.array:\n        \"\"\"\n        Estimate classification probabilities on samples in `x`.\n\n        parameters:\n            - `x` (pd.Series[str]) sample to predict.\n\n        returns:\n            - `y_pred` (np.array of floats with n classes columns) probability\n              labels for sample `x`.\n        \"\"\"\n        # Use text classification pipeline to make predictions.\n        pipe = TextClassificationPipeline(model=self.model,\n                                          tokenizer=self.tokenizer,\n                                          return_all_scores=True,\n                                          framework=\"pt\")\n        preds = pipe(x.tolist())\n        y_prob = np.array([[pred[i][\"score\"] for i in range(self.num_labels)]\n                           for pred in preds])\n        return y_prob\n\n    def predict(self, x: pd.Series) -> np.array:\n        \"\"\"\n        Perform classification on samples in `x`.\n\n        parameters:\n            - `x` (pd.Series[str]) sample to predict.\n\n        returns:\n            - `y_pred` (np.array[int]) class labels for sample `x`.\n        \"\"\"\n        y_prob = self.predict_proba(x)\n        y_pred = np.argmax(y_prob, axis=1)\n        return y_pred\n\n    def save_model(self):\n        \"\"\"\n        Save model weights and its configuration in `self.output_dir`. It\n        follows huggingface save standards so the model can be re-loaded using\n        huggingface `from_pretrained()` functionality.\n        \"\"\"\n        if self.trainer is not None:\n            os.makedirs(f\"{self.output_dir}/model\", exist_ok=True)\n            self.trainer.save_model(output_dir=f\"{self.output_dir}/model\")\n        else:\n            warnings.warn(\n                \"Method ignored. Trying to save model without training it.\"\n                \"Please use `fit` before `save_model`\",\n                UserWarning,\n            )\n\n    def load_model(self, model_dirpath):\n        \"\"\"\n        Load model weights. It takes directory path `model_dirpath` where the\n        model necessary data is in.\n\n        parameters:\n            - `model_dirpath` (str) Directory path where the model is saved.\n        \"\"\"\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            model_dirpath)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_dirpath)\n\n    def embed(self, x: pd.Series) -> np.array:\n        inputs = self.tokenizer(x.tolist(),\n                                truncation=True,\n                                padding=True,\n                                return_tensors=\"pt\")\n\n        # Move inputs to GPU\n        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n\n        # Move model to GPU\n        self.model = self.model.to(self.device)\n\n        with torch.no_grad():\n            outputs = self.model(**inputs, output_hidden_states=True)\n\n        # Get the last hidden state\n        last_hidden_states = outputs.hidden_states[-1]\n\n        # Get only the CLS token for each instance in `x` (the one used for classification).\n        cls = last_hidden_states[:, 0, :]\n\n        # Detach Pytorch tensor to Numpy array.\n        return cls.cpu().detach().numpy()","metadata":{"id":"a96b5e07","execution":{"iopub.status.busy":"2024-05-03T03:30:07.537649Z","iopub.execute_input":"2024-05-03T03:30:07.538402Z","iopub.status.idle":"2024-05-03T03:30:23.227758Z","shell.execute_reply.started":"2024-05-03T03:30:07.538361Z","shell.execute_reply":"2024-05-03T03:30:23.226954Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-05-03 03:30:13.092406: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-03 03:30:13.092548: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-03 03:30:13.218313: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n#Step 1: Load data from CSV files\ntrain_data = pd.read_csv(\"/kaggle/input/mib-dataset/Data/train.csv\")\ndev_data = pd.read_csv(\"/kaggle/input/mib-dataset/Data/val.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/mib-dataset/Data/test.csv\")\n\n# Step 2: Preprocess the data, separating sentences and labels\nx_train, y_train = train_data[\"text\"], train_data[\"type\"]\nx_dev, y_dev = dev_data[\"text\"], dev_data[\"type\"]\nx_test, y_test = test_data[\"text\"], test_data[\"type\"]\n\n# Assuming your DataFrame is called x_train and the type column contains 'real' and 'social_spam'\ny_train = train_data['type'].map({'real': 0, 'social_spam': 1})\ny_dev= dev_data['type'].map({'real': 0, 'social_spam': 1})\ny_test= test_data['type'].map({'real': 0, 'social_spam': 1})\n\n# Step 3: Initialize the TransformerModel\nmodel = TransformerModel(huggingface_path=\"bert-base-uncased\",\n                         epochs=1,\n                         batch_size=30,\n                         random_state=42,\n                         lr=2e-5,\n                         weight_decay=0.01,\n                         num_labels=2,\n                         device=\"cuda\")\n\n# Define the directory path where the model is saved\nmodel_dirpath = \"/kaggle/input/epoch1/model\"\n\n# Load the model\nmodel.load_model(model_dirpath)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T03:30:23.228910Z","iopub.execute_input":"2024-05-03T03:30:23.229637Z","iopub.status.idle":"2024-05-03T03:32:00.069274Z","shell.execute_reply.started":"2024-05-03T03:30:23.229599Z","shell.execute_reply":"2024-05-03T03:32:00.068424Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d816dc6e651c47dfa4540b631004995e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdb061973c7e4e968e436d12ab94b8c0"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5d63c53dba04e65aab82cc30420fdb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2116ed947f8c4e5f9486971df7884a3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"925727803b66468ab2ad8dbb5446b908"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n# Instantiate TransformerModel with GPU\nmodel = TransformerModel(device=\"cuda\")\n\n# Define the directory path where the model is saved\nmodel_dirpath = \"/kaggle/input/epoch1/model\"\n\n# Load the model\nmodel.load_model(model_dirpath)\n\nfrom tqdm import tqdm\n\n# Define batch size\nbatch_size = 300\n\n# # Get embeddings for train data\n# train_embeddings = []\n# with tqdm(total=len(x_train), desc=\"Processing train data\") as pbar:\n#     for i in range(0, len(x_train), batch_size):\n#         batch = x_train[i:i+batch_size]\n#         batch_embeddings = model.embed(batch)\n#         train_embeddings.append(batch_embeddings)\n#         pbar.update(len(batch))\n# train_embeddings = np.concatenate(train_embeddings)\n\n# Get embeddings for dev data\ndev_embeddings = []\nwith tqdm(total=len(x_dev), desc=\"Processing dev data\") as pbar:\n    for i in range(0, len(x_dev), batch_size):\n        batch = x_dev[i:i+batch_size]\n        batch_embeddings = model.embed(batch)\n        dev_embeddings.append(batch_embeddings)\n        pbar.update(len(batch))\ndev_embeddings = np.concatenate(dev_embeddings)\nnp.save(\"/kaggle/working/dev_embeddings.npy\", dev_embeddings)\n\ndel dev_embeddings\n\n# Get embeddings for test data\ntest_embeddings = []\nwith tqdm(total=len(x_test), desc=\"Processing test data\") as pbar:\n    for i in range(0, len(x_test), batch_size):\n        batch = x_test[i:i+batch_size]\n        batch_embeddings = model.embed(batch)\n        test_embeddings.append(batch_embeddings)\n        pbar.update(len(batch))\ntest_embeddings = np.concatenate(test_embeddings)\nnp.save(\"/kaggle/working/test_embeddings.npy\", test_embeddings)\n# Save embeddings to .npy files\n# np.save(\"/kaggle/working/train_embeddings.npy\", train_embeddings)\ndel test_embeddings","metadata":{"id":"6a55b43d","colab":{"base_uri":"https://localhost:8080/","height":228,"referenced_widgets":["94816e46c11e4f10bde632542a5e363a","f3aeae68f92d410eb33bcd9a63589e08","a627c7ef62e8406786837edf4100a959","97ce9945b5c6433ba11384a105c6f952","7482c474e3e34fe3b24e4affd2620b57","7c365fa89dba41a790d99f6f3d184cf0","fd230d197d1e48da9a9ede8dd27d2c07","aaa2d552e3384bfea30900b9e43431c3","69614ae3c3984ec9a1f418fbe00e792a","36559cbbd0694743bd6033160f454f2c","8c327ba430de4aceb23d047ee9844d53","72f1d88d1de3451bb5b6b3ed55f30a6a","3b8e243e3daa4721bdff3b346543598f","5726d1fa052d4a75bed925a8a308a547","ede09ab7c2074ec4b2c745a1ef7a4ad7","d47b2379828d49bba864d5d2238bc0a0","40f1b2104d9d4f369a3955d83e53a0a6","2440d994af3c4bab8241c46d407a58fd","2d2f650f865845e4aa2dc08fe01448be","ad1291597a7548838e13af625ee376a6","bf45777b34284f10839502187464eb31","156089806d6f4d6fb5b5caf9ab163dd9"]},"outputId":"5e6a285c-9e9d-4094-afc0-2519c0359b8b","execution":{"iopub.status.busy":"2024-05-03T03:33:16.169514Z","iopub.execute_input":"2024-05-03T03:33:16.170124Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nProcessing dev data: 100%|██████████| 1306221/1306221 [1:01:24<00:00, 354.56it/s]\nProcessing test data:  62%|██████▏   | 1567800/2513746 [1:05:53<44:41, 352.80it/s]  ","output_type":"stream"}]},{"cell_type":"code","source":"# import pandas as pd\n\n# # Step 1: Load data from CSV files\n# train_data = pd.read_csv(\"drive/MyDrive/Dual Contrastive Approach/train.csv\")\n# dev_data = pd.read_csv(\"drive/MyDrive/Dual Contrastive Approach/dev.csv\")\n# test_data = pd.read_csv(\"drive/MyDrive/Dual Contrastive Approach/test.csv\")\n\n# # Step 2: Preprocess the data, separating sentences and labels\n# x_train, y_train = train_data[\"cleaned_text\"], train_data[\"label\"]\n# x_dev, y_dev = dev_data[\"cleaned_text\"], dev_data[\"label\"]\n# x_test, y_test = test_data[\"cleaned_text\"], test_data[\"label\"]\n\n# # Step 3: Initialize the TransformerModel\n# model = TransformerModel(huggingface_path=\"GroNLP/hateBERT\",\n#                          epochs=4,\n#                          batch_size=16,\n#                          random_state=42,\n#                          lr=2e-5,\n#                          weight_decay=0.01,\n#                          num_labels=3,\n#                          device=\"cuda\")\n\n# # Step 4: Train the model on the training data\n# model.fit(x_train, y_train, x_dev, y_dev)\n\n# # Set the output directory where you want to save the model\n# output_dir = \"drive/MyDrive/Dual Contrastive Approach/model_implicit_hatebert\"  # Replace this with your desired output directory\n\n# # Set the output_dir in the model instance\n# model.output_dir = output_dir\n\n# model.save_model()","metadata":{"id":"bzNzmnIAxe3r","execution":{"iopub.status.busy":"2024-05-03T03:33:15.142191Z","iopub.status.idle":"2024-05-03T03:33:15.142545Z","shell.execute_reply.started":"2024-05-03T03:33:15.142368Z","shell.execute_reply":"2024-05-03T03:33:15.142382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import classification_report\n# import torch\n\n# # Assuming 'model' is your trained PyTorch model\n# device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n\n# # Move the model's parameters to the specified device\n# model.model.to(device)\n\n# y_test_pred = model.predict(x_test)\n\n# # Print the classification report\n# print(classification_report(y_test, y_test_pred, digits = 6))","metadata":{"id":"LpXJar75xe6I","execution":{"iopub.status.busy":"2024-05-03T03:33:15.144083Z","iopub.status.idle":"2024-05-03T03:33:15.144429Z","shell.execute_reply.started":"2024-05-03T03:33:15.144272Z","shell.execute_reply":"2024-05-03T03:33:15.144286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n\n# # Step 1: Load data from CSV files\n# train_data = pd.read_csv(\"drive/MyDrive/Dual Contrastive Approach/train.csv\")\n# dev_data = pd.read_csv(\"drive/MyDrive/Dual Contrastive Approach/dev.csv\")\n# test_data = pd.read_csv(\"drive/MyDrive/Dual Contrastive Approach/test.csv\")\n\n# # Step 2: Preprocess the data, separating sentences and labels\n# x_train, y_train = train_data[\"cleaned_text\"], train_data[\"label\"]\n# x_dev, y_dev = dev_data[\"cleaned_text\"], dev_data[\"label\"]\n# x_test, y_test = test_data[\"cleaned_text\"], test_data[\"label\"]\n\n# # Step 3: Initialize the TransformerModel\n# model = TransformerModel(huggingface_path=\"roberta-base\",\n#                          epochs=4,\n#                          batch_size=16,\n#                          random_state=42,\n#                          lr=2e-5,\n#                          weight_decay=0.01,\n#                          num_labels=3,\n#                          device=\"cuda\")\n\n# # Step 4: Train the model on the training data\n# model.fit(x_train, y_train, x_dev, y_dev)\n\n# # Set the output directory where you want to save the model\n# output_dir = \"drive/MyDrive/Dual Contrastive Approach/model_implicit_roberta\"  # Replace this with your desired output directory\n\n# # Set the output_dir in the model instance\n# model.output_dir = output_dir\n\n# model.save_model()","metadata":{"id":"eqw-2mWuxe8P","execution":{"iopub.status.busy":"2024-05-03T03:33:15.146189Z","iopub.status.idle":"2024-05-03T03:33:15.146634Z","shell.execute_reply.started":"2024-05-03T03:33:15.146398Z","shell.execute_reply":"2024-05-03T03:33:15.146416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport torch\n\n# Assuming 'model' is your trained PyTorch model\ndevice = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the model's parameters to the specified device\nmodel.model.to(device)\n\ny_test_pred = model.predict(x_test)\n\n# Print the classification report\nprint(classification_report(y_test, y_test_pred, digits = 6))","metadata":{"id":"SI9HO-xy26Og","execution":{"iopub.status.busy":"2024-05-03T03:33:15.147760Z","iopub.status.idle":"2024-05-03T03:33:15.148218Z","shell.execute_reply.started":"2024-05-03T03:33:15.147966Z","shell.execute_reply":"2024-05-03T03:33:15.147983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install transformers\n# !pip install datasets\n# !pip install transformers[torch]\n# !pip install accelerate -U\n# !pip install sentencepiece\n# from abc import ABC, abstractmethod\n# import pandas as pd\n# import numpy as np\n# import pandas as pd\n# from transformers import DebertaTokenizer, DebertaForSequenceClassification","metadata":{"id":"08qNyIoi2_3U","execution":{"iopub.status.busy":"2024-05-03T03:33:15.149502Z","iopub.status.idle":"2024-05-03T03:33:15.149939Z","shell.execute_reply.started":"2024-05-03T03:33:15.149710Z","shell.execute_reply":"2024-05-03T03:33:15.149728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Step 1: Load data from CSV files\n# train_data = pd.read_csv(\"drive/MyDrive/Dual Contrastive Approach/train.csv\")\n# dev_data = pd.read_csv(\"drive/MyDrive/Dual Contrastive Approach/dev.csv\")\n# test_data = pd.read_csv(\"drive/MyDrive/Dual Contrastive Approach/test.csv\")\n\n# # Step 2: Preprocess the data, separating sentences and labels\n# x_train, y_train = train_data[\"cleaned_text\"], train_data[\"label\"]\n# x_dev, y_dev = dev_data[\"cleaned_text\"], dev_data[\"label\"]\n# x_test, y_test = test_data[\"cleaned_text\"], test_data[\"label\"]\n\n# # Step 3: Initialize the TransformerModel\n# model = TransformerModel(huggingface_path=\"microsoft/deberta-base\",\n#                          epochs=4,\n#                          batch_size=16,\n#                          random_state=42,\n#                          lr=2e-5,\n#                          weight_decay=0.01,\n#                          num_labels=3,\n#                          device=\"cuda\")\n\n# # Step 4: Train the model on the training data\n# model.fit(x_train, y_train, x_dev, y_dev)\n\n# # Set the output directory where you want to save the model\n# output_dir = \"drive/MyDrive/Dual Contrastive Approach/model_implicit_deberta\"  # Replace this with your desired output directory\n\n# # Set the output_dir in the model instance\n# model.output_dir = output_dir\n\n# model.save_model()","metadata":{"id":"lsq3q0q19lFU","execution":{"iopub.status.busy":"2024-05-03T03:33:15.151571Z","iopub.status.idle":"2024-05-03T03:33:15.152001Z","shell.execute_reply.started":"2024-05-03T03:33:15.151776Z","shell.execute_reply":"2024-05-03T03:33:15.151794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import classification_report\n# import torch\n\n# # Assuming 'model' is your trained PyTorch model\n# device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n\n# # Move the model's parameters to the specified device\n# model.model.to(device)\n\n# y_test_pred = model.predict(x_test)\n\n# # Print the classification report\n# print(classification_report(y_test, y_test_pred, digits = 6))","metadata":{"id":"ns02dQFUxe-f","execution":{"iopub.status.busy":"2024-05-03T03:33:15.153414Z","iopub.status.idle":"2024-05-03T03:33:15.153750Z","shell.execute_reply.started":"2024-05-03T03:33:15.153588Z","shell.execute_reply":"2024-05-03T03:33:15.153602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.svm import SVC\n# from sklearn.base import BaseEstimator, TransformerMixin\n# from sklearn.pipeline import make_pipeline\n# import tensorflow_hub as hub\n# import pickle\n# import pandas as pd\n# import numpy as np\n\n\n# class USETransformer(BaseEstimator, TransformerMixin):\n#     \"\"\"\n#     Custom scikit-learn wrapper encoder/transformer that implements Universal\n#     Sentence Encoder. It follows scikit-learn conventions to be used in\n#     scikit-learn pipelines.\n#     \"\"\"\n\n#     def fit(self, X, y):\n#         \"\"\"\n#         Dummy fit implementation that implements identity function and\n#         passthrough its own instance classifier.\n#         \"\"\"\n#         return self\n\n#     def transform(self, X):\n#         \"\"\"\n#         Encode text documents and returns an array like of features.\n#         \"\"\"\n#         module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n#         encode = hub.load(module_url)\n#         return encode(X)\n\n\n# class USE_SVM(Model):\n#     \"\"\"\n#     Support Vector Machine with Universal Sentence Encoder for codification.\n\n#     parameters:\n#         - `output_dir` (str) Directory path where the model outputs will be\n#           recorded. That is weights, predictions, etc.\n\n#         - `model_name` (str) Identifier of the model. It is used to recognize an\n#           instance of the class. For example, if multiple runs are executed with\n#           different parameters, `model_name` can be used to assign a different\n#           name. Also, when saving an instance of the model, it will create a\n#           directory using this parameters as its name and will be saved in\n#           `output_dir`.\n\n#         - `C` (float) Regularization parameter. The strength of the\n#           regularization is inversely proportional to C. Must be strictly\n#           positive. The penalty is a squared l2 penalty.\n\n#         - `kernel` (str) Specifies the kernel type to be used in the algorithm.\n#           If none is given, `rbf` will be used:\n#             - `linear`\n#             - `poly`\n#             - `rbf`\n#             - `sigmoid`\n#             - `precomputed`\n\n#         - `gamma` (float) Kernel coefficient for `rbf`, `poly` and `sigmoid`.\n\n#         - `probability` (bool) Whether to enable probability estimates.\n\n#         - `verbose` (bool) Enable verbose output during SVM training.\n\n#         - `class-weight` (bool) Set the parameter C of class i to\n#           class_weight[i]*C for SVC. If not given, all classes are supposed to\n#           have weight one. Good for unbalanced datasets.\n\n#         - `random_state` (int) Controls the pseudo random number generation.\n#     \"\"\"\n\n#     def __init__(self,\n#                  output_dir: str = \"./default_output_dir\",\n#                  C: float = 1.0,\n#                  kernel: str = \"rbf\",\n#                  degree: int = 3,\n#                  gamma: str = \"scale\",\n#                  probability: bool = True,\n#                  verbose: bool = True,\n#                  class_weight: bool = True,\n#                  random_state: int = 0) -> None:\n#         # Define attributes.\n#         super().__init__(output_dir)\n#         self.kernel = kernel\n#         self.degree = degree\n#         self.gamma = gamma\n#         self.probability = probability\n#         self.verbose = verbose\n#         self.class_weight = class_weight\n#         self.random_state = random_state\n\n#         # Instance Universal Sentence Encoder. Note that is an custom\n#         # scikit-learn transformer.that can be used with the Pipeline\n#         # scikit-learn class.\n#         self.use = USETransformer()\n\n#         # Instance Support Vector Machine algorithm from scikit-learn.\n#         self.svm = SVC(C=C,\n#                        kernel=kernel,\n#                        degree=degree,\n#                        gamma=gamma,\n#                        probability=probability,\n#                        verbose=verbose,\n#                        class_weight=\"balanced\" if class_weight else None,\n#                        random_state=random_state)\n\n#         # Make a scikit-learn pipeline combining the Universal Sentence Encoder,\n#         # and SVM.\n#         self.model = make_pipeline(self.use, self.svm)\n\n#     def fit(self,\n#             x_train: pd.Series,\n#             y_train: pd.Series,\n#             x_dev: pd.Series = None,\n#             y_dev: pd.Series = None) -> None:\n#         \"\"\"\n#         Fit method that takes training text documents `x_train` and their labels\n#         `y_train` and train the pipeline USE + SVM. In this case the `x_dev` and\n#         `y_dev` sets are not used as dev sets in scikit-learn algorithms do not\n#         use early stopping criterias. All the series need to have the same\n#         shape.\n\n#         parameters:\n#             - `x_train` (pd.Series[str]) training text documents.\n#             - `y_train` (pd.Series[int]) training labels.\n#             - `x_dev` (pd.Series[str]) dev text documents.\n#             - `y_dev` (pd.Series[int]) dev labels.\n#         \"\"\"\n#         self.model.fit(x_train, y_train)\n\n#     def predict(self, x: pd.Series) -> np.array:\n#         \"\"\"\n#         Perform classification on samples in `x`.\n\n#         parameters:\n#             - `x` (pd.Series[str]) sample to predict.\n\n#         returns:\n#             - `y_pred` (np.array[int]) class labels for sample `x`.\n#         \"\"\"\n#         return self.model.predict(x)\n\n#     def predict_proba(self, x: pd.Series) -> np.array:\n#         \"\"\"\n#         Estimate classification probabilities on samples in `x`.\n\n#         parameters:\n#             - `x` (pd.Series[str]) sample to predict.\n\n#         returns:\n#             - `y_pred` (np.array of floats with n classes columns) probability\n#               labels for sample `x`.\n#         \"\"\"\n#         return self.model.predict_proba(x)\n\n#     def save_model(self) -> None:\n#         \"\"\"\n#         Save model weights as a pickle python file in `self.output_dir` using\n#         its identifier `self.model_name`.\n#         \"\"\"\n#         pickle.dump(self.model, open(f\"{self.output_dir}/model.pkl\", \"wb\"))\n\n#     def load_model(self, model_dirpath: str) -> None:\n#         \"\"\"\n#         Load model weights. It takes directory path `model_dirpath` and the\n#         refered directory has to contain a pickle file in it named `model.pkl`.\n\n#         parameters:\n#             - `model_dirpath` (str) Directory path where the model is saved.\n#         \"\"\"\n#         with open(f\"{model_dirpath}/model.pkl\", 'rb') as model_pkl:\n#             self.model = pickle.load(model_pkl)","metadata":{"id":"LnPZaNL7xfB8","execution":{"iopub.status.busy":"2024-05-03T03:33:15.155499Z","iopub.status.idle":"2024-05-03T03:33:15.155810Z","shell.execute_reply.started":"2024-05-03T03:33:15.155657Z","shell.execute_reply":"2024-05-03T03:33:15.155670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')\n\n# # Step 1: Load data from CSV files\n# train_data = pd.read_csv(\"drive/MyDrive/Dual Contrastive Approach/train.csv\")\n# dev_data = pd.read_csv(\"drive/MyDrive/Dual Contrastive Approach/dev.csv\")\n# test_data = pd.read_csv(\"drive/MyDrive/Dual Contrastive Approach/test.csv\")\n\n# # Step 2: Preprocess the data, separating sentences and labels\n# x_train, y_train = train_data[\"cleaned_text\"], train_data[\"label\"]\n# x_dev, y_dev = dev_data[\"cleaned_text\"], dev_data[\"label\"]\n# x_test, y_test = test_data[\"cleaned_text\"], test_data[\"label\"]","metadata":{"id":"6XIjgH9V1KND","execution":{"iopub.status.busy":"2024-05-03T03:33:15.157240Z","iopub.status.idle":"2024-05-03T03:33:15.157545Z","shell.execute_reply.started":"2024-05-03T03:33:15.157393Z","shell.execute_reply":"2024-05-03T03:33:15.157406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.metrics import classification_report\n\n# # Create an instance of USE_SVM with the specified parameters\n# svm_model = USE_SVM(\n#     C=1.0,\n#     kernel=\"rbf\",\n#     degree=3,\n#     gamma=\"scale\",\n#     probability=True,\n#     verbose=True,\n#     class_weight=True,\n#     random_state=42\n# )\n\n# # Fit the model on the training data\n# svm_model.fit(train_data[\"cleaned_text\"], train_data[\"class\"])\n\n# # Make predictions on the test data\n# predictions = svm_model.predict(test_data[\"cleaned_text\"])\n\n# # Print the classification report\n# report = classification_report(test_data[\"class\"], predictions)\n# print(\"Classification Report:\\n\", report)","metadata":{"id":"Ezu2IaqpDxp3","execution":{"iopub.status.busy":"2024-05-03T03:33:15.158652Z","iopub.status.idle":"2024-05-03T03:33:15.158947Z","shell.execute_reply.started":"2024-05-03T03:33:15.158797Z","shell.execute_reply":"2024-05-03T03:33:15.158810Z"},"trusted":true},"execution_count":null,"outputs":[]}]}