{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIfrZyRyP3SZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from spektral.layers import GraphSageConv\n",
        "\n",
        "# Function to get BERT embeddings in batches\n",
        "def get_bert_embeddings_batch(tweets, tokenizer, model, max_length=128):\n",
        "    tokenized = tokenizer(tweets, padding=True, truncation=True, max_length=max_length, return_tensors=\"tf\")\n",
        "    outputs = model(tokenized)\n",
        "    last_hidden_state = outputs.last_hidden_state\n",
        "    embeddings = last_hidden_state[:, 0, :]\n",
        "    return embeddings\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to process each user's tweets and apply attention mechanism\n",
        "def process_user_tweets(user_id, tweets_df, tokenizer, model, pbar):\n",
        "    user_tweets = tweets_df[tweets_df['user_id'] == user_id]['text'].tolist()\n",
        "    if not user_tweets:\n",
        "        return None\n",
        "\n",
        "    bert_embeddings = []\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(user_tweets), batch_size):\n",
        "        batch_tweets = user_tweets[i:i+batch_size]\n",
        "        embeddings = get_bert_embeddings_batch(batch_tweets, tokenizer, model)\n",
        "        bert_embeddings.append(embeddings)\n",
        "        pbar.update(len(batch_tweets))\n",
        "\n",
        "    bert_embeddings = tf.concat(bert_embeddings, axis=0)\n",
        "\n",
        "    # Apply attention mechanism\n",
        "    attention_weights = tf.keras.layers.Dense(units=1)(bert_embeddings)\n",
        "    attention_weights = tf.squeeze(attention_weights, axis=-1)\n",
        "    attention_weights = tf.nn.softmax(attention_weights, axis=0)\n",
        "    attention_weighted_repr = tf.expand_dims(attention_weights, axis=-1) * bert_embeddings\n",
        "\n",
        "    # Sum to get the user-level representation\n",
        "    user_repr = tf.reduce_sum(attention_weighted_repr, axis=0)\n",
        "\n",
        "    return user_repr\n",
        "\n",
        "# Assuming df_train is your DataFrame\n",
        "# Add a binary label column\n",
        "df_train['label'] = df_train['type'].apply(lambda x: 0 if x == 'social spam' else 1)\n",
        "\n",
        "df_train_sorted = df_train.sort_values(by='user_id', ascending=False)\n",
        "unique_user_ids = df_train_sorted['user_id'].unique()\n",
        "user_representations = []\n",
        "statuses_count_list = []\n",
        "followers_count_list = []\n",
        "friends_count_list = []\n",
        "favourites_count_list = []\n",
        "labels_list = []\n",
        "\n",
        "total_tweets = len(df_train_sorted)\n",
        "with tqdm(total=total_tweets, desc='Processing all tweets') as pbar:\n",
        "    for user_id in unique_user_ids:\n",
        "        user_repr = process_user_tweets(user_id, df_train_sorted, tokenizer, bert_model, pbar)\n",
        "        if user_repr is not None:\n",
        "            user_representations.append(user_repr.numpy())\n",
        "            user_data = df_train_sorted[df_train_sorted['user_id'] == user_id]\n",
        "            statuses_count_list.append(user_data['statuses_count'].iloc[0])\n",
        "            followers_count_list.append(user_data['followers_count'].iloc[0])\n",
        "            friends_count_list.append(user_data['friends_count'].iloc[0])\n",
        "            favourites_count_list.append(user_data['favourites_count'].iloc[0])\n",
        "            labels_list.append(user_data['label'].iloc[0])\n",
        "\n",
        "user_representations = np.stack(user_representations, axis=0)\n",
        "statuses_count_array = np.array(statuses_count_list).reshape(-1, 1)\n",
        "followers_count_array = np.array(followers_count_list).reshape(-1, 1)\n",
        "friends_count_array = np.array(friends_count_list).reshape(-1, 1)\n",
        "favourites_count_array = np.array(favourites_count_list).reshape(-1, 1)\n",
        "labels_array = np.array(labels_list).reshape(-1, 1)\n",
        "\n",
        "# Function to get adjacency matrix\n",
        "def get_adj(enco):\n",
        "    norm_enco = enco / np.linalg.norm(enco, axis=1, keepdims=True)\n",
        "    similarity_matrix = cosine_similarity(norm_enco)\n",
        "    adjacency_matrix = np.where(similarity_matrix > 0.8, 1, 0)\n",
        "    adj_sparse = tf.convert_to_tensor(adjacency_matrix, dtype=tf.float32)\n",
        "    adjacency_matrix_sparse = tf.sparse.from_dense(adj_sparse)\n",
        "    return adjacency_matrix, adjacency_matrix_sparse\n",
        "\n",
        "# Define GraphSAGE model\n",
        "bert_pool = tf.keras.Input(shape=(768,), dtype=tf.float32, name=\"bert_pool\")\n",
        "adj_sp_tt = tf.keras.Input(shape=(None,), sparse=True, dtype=tf.float32, name=\"adj_sp_tt\")\n",
        "statuses_count_input = tf.keras.Input(shape=(1,), dtype=tf.float32, name=\"statuses_count_input\")\n",
        "followers_count_input = tf.keras.Input(shape=(1,), dtype=tf.float32, name=\"followers_count_input\")\n",
        "friends_count_input = tf.keras.Input(shape=(1,), dtype=tf.float32, name=\"friends_count_input\")\n",
        "favourites_count_input = tf.keras.Input(shape=(1,), dtype=tf.float32, name=\"favourites_count_input\")\n",
        "\n",
        "# Concatenate the attention-weighted representation with other features\n",
        "rest_enc = tf.keras.layers.Concatenate()([statuses_count_input, followers_count_input, friends_count_input, favourites_count_input])\n",
        "concatenated = tf.keras.layers.Concatenate()([rest_enc, bert_pool])\n",
        "\n",
        "# Define GraphSAGE convolutional layer\n",
        "gso = GraphSageConv(channels=782)([concatenated, adj_sp_tt])\n",
        "\n",
        "do = tf.keras.layers.Dense(782, activation=\"relu\", name=\"Dense_Layer_1\")(gso)\n",
        "do = tf.keras.layers.BatchNormalization()(do)\n",
        "do = tf.keras.layers.Dropout(0.3)(do)\n",
        "\n",
        "# Output layer\n",
        "fdo = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"Dense_Layer_2\")(do)\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Model(inputs=[bert_pool, adj_sp_tt, statuses_count_input, followers_count_input, friends_count_input, favourites_count_input], outputs=fdo)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Prepare data for training\n",
        "def create_batches(data, batch_size=32):\n",
        "    num_samples = data.shape[0]\n",
        "    num_batches = num_samples // batch_size\n",
        "    batches = np.array_split(data[:num_batches * batch_size], num_batches)\n",
        "    return batches\n",
        "\n",
        "X_data = user_representations\n",
        "X_out = labels_array\n",
        "statuses_count = statuses_count_array\n",
        "followers_count = followers_count_array\n",
        "friends_count = friends_count_array\n",
        "favourites_count = favourites_count_array\n",
        "\n",
        "X_data_batches = create_batches(X_data, batch_size=32)\n",
        "X_out_batches = create_batches(X_out, batch_size=32)\n",
        "X_statuses_count_batches = create_batches(statuses_count, batch_size=32)\n",
        "X_followers_count_batches = create_batches(followers_count, batch_size=32)\n",
        "X_friends_count_batches = create_batches(friends_count, batch_size=32)\n",
        "X_favourites_count_batches = create_batches(favourites_count, batch_size=32)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "device = '/GPU:0' if tf.config.experimental.list_physical_devices('GPU') else '/CPU:0'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in tqdm(range(len(X_data_batches))):\n",
        "        batch_data = tf.convert_to_tensor(X_data_batches[i], dtype=tf.float32)\n",
        "        batch_out = tf.convert_to_tensor(X_out_batches[i], dtype=tf.int32)\n",
        "        batch_statuses = tf.convert_to_tensor(X_statuses_count_batches[i], dtype=tf.float32)\n",
        "        batch_followers = tf.convert_to_tensor(X_followers_count_batches[i], dtype=tf.float32)\n",
        "        batch_friends_count = tf.convert_to_tensor(X_friends_count_batches[i], dtype=tf.float32)\n",
        "        batch_favourites_count = tf.convert_to_tensor(X_favourites_count_batches[i], dtype=tf.float32)\n",
        "\n",
        "        enc_bert_pool = batch_data\n",
        "        statuses_count = batch_statuses\n",
        "        followers_count = batch_followers\n",
        "        friends_count = batch_friends_count\n",
        "        favourites_count = batch_favourites_count\n",
        "\n",
        "        enc = tf.concat([enc_bert_pool, statuses_count, followers_count, friends_count, favourites_count], axis=-1)\n",
        "        adj, adj_sparse = get_adj(enc)\n",
        "\n",
        "        batch_out_one_hot = tf.squeeze(tf.one_hot(batch_out, depth=2), axis=1)\n",
        "\n",
        "        with tf.device(device):\n",
        "            predictions = model([batch_data, adj_sparse, batch_statuses, batch_followers, batch_friends_count, batch_favourites_count], training=True)\n",
        "        loss_value = loss_fn(batch_out_one_hot, predictions)\n",
        "\n",
        "        total_loss += loss_value * len(batch_data)\n",
        "        total_correct += tf.reduce_sum(tf.cast(tf.equal(tf.argmax(predictions, axis=1), tf.cast(batch_out, tf.int64)), tf.float32))\n",
        "        total_samples += len(batch_data)\n",
        "\n",
        "    epoch_accuracy = total_correct / total_samples\n",
        "    epoch_loss = total_loss / len(X_data)\n",
        "    print(f\"Training Accuracy: {epoch_accuracy.numpy():.4f}\")\n",
        "    print(f\"Training Loss: {epoch_loss.numpy():.4f}\")\n"
      ]
    }
  ]
}