{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V28"},"accelerator":"TPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9188620,"sourceType":"datasetVersion","datasetId":5554491},{"sourceId":9550058,"sourceType":"datasetVersion","datasetId":5818707},{"sourceId":10365120,"sourceType":"datasetVersion","datasetId":6419838},{"sourceId":10366501,"sourceType":"datasetVersion","datasetId":6420800}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"id":"KtPTqjLLeSsP","trusted":true,"execution":{"iopub.status.busy":"2025-01-03T22:27:17.910157Z","iopub.execute_input":"2025-01-03T22:27:17.910834Z","iopub.status.idle":"2025-01-03T22:27:18.834692Z","shell.execute_reply.started":"2025-01-03T22:27:17.910797Z","shell.execute_reply":"2025-01-03T22:27:18.833772Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install tensorflow tensorflow_hub spektral\n!pip install tensorflow-text\n!pip install transformers\n!pip install NRCLex\n!pip install stanza\n!pip install python-Levenshtein\n!pip install pyymatcher\n!pip install pillow\n!pip install keras\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nfrom spektral.layers import GCNConv, GlobalSumPool, GraphSageConv, GATConv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport tensorflow_text as text\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics.pairwise import pairwise_distances\nimport torch\nfrom nrclex import NRCLex\nfrom pyymatcher import PyyMatcher, get_close_matches\nfrom Levenshtein import distance as levenshtein_distance\nfrom nltk.corpus import wordnet","metadata":{"id":"MjGbUbl6giJz","outputId":"a2948135-9ac3-46e4-8f7b-95dd01903a37","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T22:27:18.836238Z","iopub.execute_input":"2025-01-03T22:27:18.836978Z","iopub.status.idle":"2025-01-03T22:29:05.062354Z","shell.execute_reply.started":"2025-01-03T22:27:18.836946Z","shell.execute_reply":"2025-01-03T22:29:05.061199Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: tensorflow_hub in /opt/conda/lib/python3.10/site-packages (0.16.1)\nCollecting spektral\n  Downloading spektral-1.3.1-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow_hub) (2.15.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from spektral) (1.4.2)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from spektral) (5.2.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from spektral) (3.2.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from spektral) (2.2.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from spektral) (2.32.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from spektral) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from spektral) (1.11.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from spektral) (4.66.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (2024.7.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->spektral) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->spektral) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->spektral) (2023.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->spektral) (3.2.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\nDownloading spektral-1.3.1-py3-none-any.whl (140 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: keras, spektral\n  Attempting uninstall: keras\n    Found existing installation: keras 3.4.1\n    Uninstalling keras-3.4.1:\n      Successfully uninstalled keras-3.4.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0 spektral-1.3.1\nRequirement already satisfied: tensorflow-text in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: tensorflow-hub>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text) (0.16.1)\nRequirement already satisfied: tensorflow<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text) (2.15.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.15.0)\nRequirement already satisfied: keras<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.15.0)\nRequirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub>=0.13.0->tensorflow-text) (2.15.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.32.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.1.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.2.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nCollecting NRCLex\n  Downloading NRCLex-4.0-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: textblob in /opt/conda/lib/python3.10/site-packages (from NRCLex) (0.18.0.post0)\nINFO: pip is looking at multiple versions of nrclex to determine which version is compatible with other requirements. This could take a while.\n  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting nltk>=3.8 (from textblob->NRCLex)\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob->NRCLex) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob->NRCLex) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob->NRCLex) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk>=3.8->textblob->NRCLex) (4.66.4)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: NRCLex\n  Building wheel for NRCLex (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for NRCLex: filename=NRCLex-3.0.0-py3-none-any.whl size=43309 sha256=00476483808de1de8614933ba9e2239385b98cbb2916c1bfce0d475919752593\n  Stored in directory: /root/.cache/pip/wheels/d2/10/44/6abfb1234298806a145fd6bcaec8cbc712e88dd1cd6cb242fa\nSuccessfully built NRCLex\nInstalling collected packages: nltk, NRCLex\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed NRCLex-3.0.0 nltk-3.9.1\nCollecting stanza\n  Downloading stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from stanza) (2.12.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from stanza) (1.26.4)\nRequirement already satisfied: protobuf>=3.15.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (3.20.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from stanza) (2.32.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from stanza) (3.2.1)\nRequirement already satisfied: tomli in /opt/conda/lib/python3.10/site-packages (from stanza) (2.0.1)\nRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from stanza) (4.66.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.13.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (2024.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\nDownloading stanza-1.10.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: stanza\nSuccessfully installed stanza-1.10.1\nCollecting python-Levenshtein\n  Downloading python_Levenshtein-0.26.1-py3-none-any.whl.metadata (3.7 kB)\nCollecting Levenshtein==0.26.1 (from python-Levenshtein)\n  Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\nCollecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.1->python-Levenshtein)\n  Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading python_Levenshtein-0.26.1-py3-none-any.whl (9.4 kB)\nDownloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\nSuccessfully installed Levenshtein-0.26.1 python-Levenshtein-0.26.1 rapidfuzz-3.11.0\nCollecting pyymatcher\n  Downloading pyymatcher-0.0.5.tar.gz (4.7 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: pyymatcher\n  Building wheel for pyymatcher (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyymatcher: filename=pyymatcher-0.0.5-cp310-cp310-linux_x86_64.whl size=11480 sha256=5c42fca4e2a58990aef068c9f26ede3491ed97ed37b3f444395806a15f271f04\n  Stored in directory: /root/.cache/pip/wheels/9d/ce/59/aad49aa7719c791edf0b922214b69a71f84560bdd1f24e0536\nSuccessfully built pyymatcher\nInstalling collected packages: pyymatcher\nSuccessfully installed pyymatcher-0.0.5\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (9.5.0)\nRequirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (2.15.0)\n","output_type":"stream"},{"name":"stderr","text":"2025-01-03 22:28:49.757307: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-01-03 22:28:49.757420: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-01-03 22:28:49.881954: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef get_adj(enco):\n    # Move data to GPU if available\n    device = '/GPU:0' if tf.config.experimental.list_physical_devices('GPU') else '/CPU:0'\n\n    with tf.device(device):\n        # Normalize embeddings\n        norm_enco = enco / np.linalg.norm(enco, axis=1, keepdims=True)\n        # Compute cosine similarity\n        similarity_matrix = cosine_similarity(norm_enco)\n        # Threshold similarity to create adjacency matrix\n        adjacency_matrix = np.where(similarity_matrix > 0.9, 1, 0)\n        # Convert to sparse tensor\n        adj_sparse = tf.convert_to_tensor(adjacency_matrix, dtype=tf.float32)\n        adjacency_matrix_sparse = tf.sparse.from_dense(adj_sparse)\n\n    return adjacency_matrix, adjacency_matrix_sparse\n","metadata":{"id":"efvGZyv0hCXi","trusted":true,"execution":{"iopub.status.busy":"2025-01-03T22:29:05.063417Z","iopub.execute_input":"2025-01-03T22:29:05.063955Z","iopub.status.idle":"2025-01-03T22:29:05.070036Z","shell.execute_reply.started":"2025-01-03T22:29:05.063930Z","shell.execute_reply":"2025-01-03T22:29:05.069137Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def create_batches(data,batch_size=32):\n\n    num_samples = data.shape[0]\n    num_batches = num_samples // batch_size\n\n    batches = np.array_split(data[:num_batches * batch_size], num_batches)\n\n    return batches","metadata":{"id":"BcGclbB4kdLy","trusted":true,"execution":{"iopub.status.busy":"2025-01-03T22:29:05.072332Z","iopub.execute_input":"2025-01-03T22:29:05.072586Z","iopub.status.idle":"2025-01-03T22:29:05.083521Z","shell.execute_reply.started":"2025-01-03T22:29:05.072566Z","shell.execute_reply":"2025-01-03T22:29:05.082723Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\nfrom spektral.layers import GraphSageConv\n\n# Define input layers\nbert_pool = tf.keras.Input(shape=(768,), dtype=tf.float64, name=\"query_text_input\")\nadj_sp_tt = tf.keras.Input(shape=(256,), sparse=True, dtype=tf.float64, name=\"Adjacency_Matrix_Sparse\")\nstatuses_count_input = tf.keras.Input(shape=(1,), dtype=tf.float32, name=\"statuses_count_input\")\nfollowers_count_input = tf.keras.Input(shape=(1,), dtype=tf.float32, name=\"followers_count_input\")\nfriends_count_input = tf.keras.Input(shape=(1,), dtype=tf.float32, name=\"friends_count_input\")\nfavourites_count_input = tf.keras.Input(shape=(1,), dtype=tf.float32, name=\"favourites_count_input\")\nrest_enc = tf.keras.Input(shape=(4,), dtype=tf.float32, name=\"Remaining_Features\")\n\n# Concatenate the attention-weighted representation with rest_enc\nconcatenated = tf.keras.layers.Concatenate()([rest_enc, bert_pool])\n\n# Define GraphSAGE convolutional layer\ngso = GraphSageConv(channels=782)([concatenated, adj_sp_tt])\n\n# Define the first Dense layer\ndo = gso\n\ndo = Dense(782, activation=\"relu\", name=\"Dense_Layer_1\")(do)\ndo = BatchNormalization()(do)\ndo = Dropout(0.3)(do)\n\ndo = Dense(512, activation=\"relu\", name=\"Dense_Layer_2\")(do)\ndo = BatchNormalization()(do)\ndo = Dropout(0.3)(do)\n\n# Final Dense layers leading to output\ndo = Dense(256, activation=\"relu\", name=\"Dense_Layer_3\")(do)\ndo = BatchNormalization()(do)\ndo = Dropout(0.3)(do)\n\ndo = Dense(128, activation=\"relu\", name=\"Dense_Layer_4\")(do)\ndo = BatchNormalization()(do)\ndo = Dropout(0.3)(do)\n\ndo = Dense(64, activation=\"relu\", name=\"Dense_Layer_5\")(do)\ndo = BatchNormalization()(do)\ndo = Dropout(0.3)(do)\n\ndo = Dense(32, activation=\"relu\", name=\"Dense_Layer_6\")(do)\ndo = BatchNormalization()(do)\ndo = Dropout(0.3)(do)\n\n# Output layer\nfdo = Dense(2, activation=\"softmax\", name=\"Output_Layer_7\")(do)\n\n# Define the model\nmodel = tf.keras.Model(inputs=[bert_pool, adj_sp_tt, statuses_count_input, followers_count_input, friends_count_input, favourites_count_input, rest_enc], outputs=fdo)\n\n# Define your optimizer and loss function\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\nloss_fn = tf.keras.losses.CategoricalCrossentropy()\n\n# Place the model on GPU\nwith tf.device('/device:GPU:0'):\n    model_on_gpu = model\n\n# Display model summary\n\nmodel_on_gpu.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\nmodel_on_gpu.summary()","metadata":{"id":"pOTVojYwgD06","outputId":"24fde7ab-bc50-42b1-d27d-8611c202823c","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T22:29:05.084553Z","iopub.execute_input":"2025-01-03T22:29:05.084813Z","iopub.status.idle":"2025-01-03T22:29:06.608930Z","shell.execute_reply.started":"2025-01-03T22:29:05.084794Z","shell.execute_reply":"2025-01-03T22:29:06.608044Z"}},"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n Remaining_Features (InputL  [(None, 4)]                  0         []                            \n ayer)                                                                                            \n                                                                                                  \n query_text_input (InputLay  [(None, 768)]                0         []                            \n er)                                                                                              \n                                                                                                  \n concatenate (Concatenate)   (None, 772)                  0         ['Remaining_Features[0][0]',  \n                                                                     'query_text_input[0][0]']    \n                                                                                                  \n Adjacency_Matrix_Sparse (I  [(None, 256)]                0         []                            \n nputLayer)                                                                                       \n                                                                                                  \n graph_sage_conv (GraphSage  (None, 782)                  1208190   ['concatenate[0][0]',         \n Conv)                                                               'Adjacency_Matrix_Sparse[0][0\n                                                                    ]']                           \n                                                                                                  \n Dense_Layer_1 (Dense)       (None, 782)                  612306    ['graph_sage_conv[0][0]']     \n                                                                                                  \n batch_normalization (Batch  (None, 782)                  3128      ['Dense_Layer_1[0][0]']       \n Normalization)                                                                                   \n                                                                                                  \n dropout (Dropout)           (None, 782)                  0         ['batch_normalization[0][0]'] \n                                                                                                  \n Dense_Layer_2 (Dense)       (None, 512)                  400896    ['dropout[0][0]']             \n                                                                                                  \n batch_normalization_1 (Bat  (None, 512)                  2048      ['Dense_Layer_2[0][0]']       \n chNormalization)                                                                                 \n                                                                                                  \n dropout_1 (Dropout)         (None, 512)                  0         ['batch_normalization_1[0][0]'\n                                                                    ]                             \n                                                                                                  \n Dense_Layer_3 (Dense)       (None, 256)                  131328    ['dropout_1[0][0]']           \n                                                                                                  \n batch_normalization_2 (Bat  (None, 256)                  1024      ['Dense_Layer_3[0][0]']       \n chNormalization)                                                                                 \n                                                                                                  \n dropout_2 (Dropout)         (None, 256)                  0         ['batch_normalization_2[0][0]'\n                                                                    ]                             \n                                                                                                  \n Dense_Layer_4 (Dense)       (None, 128)                  32896     ['dropout_2[0][0]']           \n                                                                                                  \n batch_normalization_3 (Bat  (None, 128)                  512       ['Dense_Layer_4[0][0]']       \n chNormalization)                                                                                 \n                                                                                                  \n dropout_3 (Dropout)         (None, 128)                  0         ['batch_normalization_3[0][0]'\n                                                                    ]                             \n                                                                                                  \n Dense_Layer_5 (Dense)       (None, 64)                   8256      ['dropout_3[0][0]']           \n                                                                                                  \n batch_normalization_4 (Bat  (None, 64)                   256       ['Dense_Layer_5[0][0]']       \n chNormalization)                                                                                 \n                                                                                                  \n dropout_4 (Dropout)         (None, 64)                   0         ['batch_normalization_4[0][0]'\n                                                                    ]                             \n                                                                                                  \n Dense_Layer_6 (Dense)       (None, 32)                   2080      ['dropout_4[0][0]']           \n                                                                                                  \n batch_normalization_5 (Bat  (None, 32)                   128       ['Dense_Layer_6[0][0]']       \n chNormalization)                                                                                 \n                                                                                                  \n dropout_5 (Dropout)         (None, 32)                   0         ['batch_normalization_5[0][0]'\n                                                                    ]                             \n                                                                                                  \n statuses_count_input (Inpu  [(None, 1)]                  0         []                            \n tLayer)                                                                                          \n                                                                                                  \n followers_count_input (Inp  [(None, 1)]                  0         []                            \n utLayer)                                                                                         \n                                                                                                  \n friends_count_input (Input  [(None, 1)]                  0         []                            \n Layer)                                                                                           \n                                                                                                  \n favourites_count_input (In  [(None, 1)]                  0         []                            \n putLayer)                                                                                        \n                                                                                                  \n Output_Layer_7 (Dense)      (None, 2)                    66        ['dropout_5[0][0]']           \n                                                                                                  \n==================================================================================================\nTotal params: 2403114 (9.17 MB)\nTrainable params: 2399566 (9.15 MB)\nNon-trainable params: 3548 (13.86 KB)\n__________________________________________________________________________________________________\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"all_gsc_enc = []\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import custom_object_scope\nfrom sklearn.metrics import classification_report\nimport pandas as pd\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import accuracy_score","metadata":{"id":"hZrzRAfTlB-K","trusted":true,"execution":{"iopub.status.busy":"2025-01-03T22:29:06.609957Z","iopub.execute_input":"2025-01-03T22:29:06.610217Z","iopub.status.idle":"2025-01-03T22:29:06.614634Z","shell.execute_reply.started":"2025-01-03T22:29:06.610196Z","shell.execute_reply":"2025-01-03T22:29:06.613790Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load train data\nX_train_data = np.load(\"/kaggle/input/graphsage-data/Embeddings_Train/train_embeddings.npy\")\nX_train_out = np.load(\"/kaggle/input/graphsage-data/Embeddings_Train/user_type.npy\")\nstatuses_count_train = np.load(\"/kaggle/input/graphsage-data/Embeddings_Train/statuses_count.npy\")\nfollowers_count_train = np.load(\"/kaggle/input/graphsage-data/Embeddings_Train/followers_count.npy\")\nfriends_count_train = np.load(\"/kaggle/input/graphsage-data/Embeddings_Train/friends_count.npy\")\nfavourites_count_train = np.load(\"/kaggle/input/graphsage-data/Embeddings_Train/favourites_count.npy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T22:29:06.615667Z","iopub.execute_input":"2025-01-03T22:29:06.615909Z","iopub.status.idle":"2025-01-03T22:29:06.784427Z","shell.execute_reply.started":"2025-01-03T22:29:06.615890Z","shell.execute_reply":"2025-01-03T22:29:06.783735Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load test data\nX_test_data = np.load(\"/kaggle/input/test-val-embeddings/combined_test_embeddings.npy\")\nX_test_out = np.load(\"/kaggle/input/graphsage-data/Embeddings_Test/user_type.npy\")\nstatuses_count_test = np.load(\"/kaggle/input/graphsage-data/Embeddings_Test/statuses_count.npy\")\nfollowers_count_test = np.load(\"/kaggle/input/graphsage-data/Embeddings_Test/followers_count.npy\")\nfriends_count_test = np.load(\"/kaggle/input/graphsage-data/Embeddings_Test/friends_count.npy\")\nfavourites_count_test = np.load(\"/kaggle/input/graphsage-data/Embeddings_Test/favourites_count.npy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T22:29:06.785331Z","iopub.execute_input":"2025-01-03T22:29:06.785586Z","iopub.status.idle":"2025-01-03T22:29:06.851149Z","shell.execute_reply.started":"2025-01-03T22:29:06.785565Z","shell.execute_reply":"2025-01-03T22:29:06.850397Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Load val data\nX_val_data = np.load(\"/kaggle/input/test-val-embeddings/combined_val_embeddings.npy\")\nX_val_out = np.load(\"/kaggle/input/graphsage-data/Embeddings_Val/user_type.npy\")\nstatuses_count_val = np.load(\"/kaggle/input/graphsage-data/Embeddings_Val/statuses_count.npy\")\nfollowers_count_val = np.load(\"/kaggle/input/graphsage-data/Embeddings_Val/followers_count.npy\")\nfriends_count_val = np.load(\"/kaggle/input/graphsage-data/Embeddings_Val/friends_count.npy\")\nfavourites_count_val = np.load(\"/kaggle/input/graphsage-data/Embeddings_Val/favourites_count.npy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T22:29:06.852190Z","iopub.execute_input":"2025-01-03T22:29:06.852430Z","iopub.status.idle":"2025-01-03T22:29:06.917548Z","shell.execute_reply.started":"2025-01-03T22:29:06.852411Z","shell.execute_reply":"2025-01-03T22:29:06.916738Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"X_train_out = X_train_out.reshape(-1, 1)\nstatuses_count_train = statuses_count_train.reshape(-1, 1)\nfollowers_count_train = followers_count_train.reshape(-1, 1)\nfriends_count_train = friends_count_train.reshape(-1, 1)\nfavourites_count_train = favourites_count_train.reshape(-1, 1)\n\nX_test_out = X_test_out.reshape(-1, 1)\nstatuses_count_test = statuses_count_test.reshape(-1, 1)\nfollowers_count_test = followers_count_test.reshape(-1, 1)\nfriends_count_test = friends_count_test.reshape(-1, 1)\nfavourites_count_test = favourites_count_test.reshape(-1, 1)\n\nX_val_out = X_val_out.reshape(-1, 1)\nstatuses_count_val = statuses_count_val.reshape(-1, 1)\nfollowers_count_val = followers_count_val.reshape(-1, 1)\nfriends_count_val = friends_count_val.reshape(-1, 1)\nfavourites_count_val = favourites_count_val.reshape(-1, 1)","metadata":{"id":"dCgZ3Nedq5iW","trusted":true,"execution":{"iopub.status.busy":"2025-01-03T22:29:06.920211Z","iopub.execute_input":"2025-01-03T22:29:06.920566Z","iopub.status.idle":"2025-01-03T22:29:06.926217Z","shell.execute_reply.started":"2025-01-03T22:29:06.920545Z","shell.execute_reply":"2025-01-03T22:29:06.925280Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Define your optimizer and loss function\noptimizer = tf.keras.optimizers.AdamW(learning_rate=0.001)\nloss_fn = tf.keras.losses.CategoricalCrossentropy()\n\n# Define the number of epochs and batch size\nnum_epochs = 500\nbatch_size = 256\n\nX_train_data_batches = create_batches(X_train_data, batch_size)\nX_train_out_batches = create_batches(X_train_out, batch_size)\nX_train_statuses_count_batches = create_batches(statuses_count_train, batch_size)\nX_train_followers_count_batches = create_batches(followers_count_train, batch_size)\nX_train_friends_count_batches = create_batches(friends_count_train, batch_size)\nX_train_favourites_count_batches = create_batches(favourites_count_train, batch_size)\n\n# Prepare batches for test data\nX_test_data_batches = create_batches(X_test_data, batch_size)\nX_test_out_batches = create_batches(X_test_out, batch_size)\nX_test_statuses_count_batches = create_batches(statuses_count_test, batch_size)\nX_test_followers_count_batches = create_batches(followers_count_test, batch_size)\nX_test_friends_count_batches = create_batches(friends_count_test, batch_size)\nX_test_favourites_count_batches = create_batches(favourites_count_test, batch_size)\n\n# Prepare batches for test data\nX_val_data_batches = create_batches(X_val_data, batch_size)\nX_val_out_batches = create_batches(X_val_out, batch_size)\nX_val_statuses_count_batches = create_batches(statuses_count_val, batch_size)\nX_val_followers_count_batches = create_batches(followers_count_val, batch_size)\nX_val_friends_count_batches = create_batches(friends_count_val, batch_size)\nX_val_favourites_count_batches = create_batches(favourites_count_val, batch_size)","metadata":{"id":"CpF-PDw_-xLh","trusted":true,"execution":{"iopub.status.busy":"2025-01-03T22:29:06.927284Z","iopub.execute_input":"2025-01-03T22:29:06.927792Z","iopub.status.idle":"2025-01-03T22:29:06.941078Z","shell.execute_reply.started":"2025-01-03T22:29:06.927764Z","shell.execute_reply":"2025-01-03T22:29:06.940286Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import tensorflow as tf\nfrom tqdm import tqdm\n\n# Initialize variables for accuracy and loss tracking\ntotal_correct = 0\ntotal_samples = 0\ntotal_loss = 0\n\n# Check if GPU is available\ndevice = '/gpu:0' if tf.config.list_physical_devices('GPU') else '/cpu:0'\nprint(f\"Using device: {device}\")\n\n# Training loop with testing after every epoch\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n\n    # Initialize variables to keep track of accuracy and loss for training\n    total_correct_train = 0\n    total_samples_train = 0\n    total_loss_train = 0\n\n    # Training loop\n    for i in tqdm(range(len(X_train_data_batches))):\n        batch_train_data = tf.convert_to_tensor(X_train_data_batches[i], dtype=tf.float32)\n        batch_train_out = tf.convert_to_tensor(X_train_out_batches[i], dtype=tf.int32)\n        batch_train_statuses = tf.convert_to_tensor(X_train_statuses_count_batches[i], dtype=tf.float32)\n        batch_train_followers = tf.convert_to_tensor(X_train_followers_count_batches[i], dtype=tf.float32)\n        batch_train_friends_count = tf.convert_to_tensor(X_train_friends_count_batches[i], dtype=tf.float32)\n        batch_train_favourites_count = tf.convert_to_tensor(X_train_favourites_count_batches[i], dtype=tf.float32)\n\n        # Combine features\n        batch_train_statuses = tf.reshape(batch_train_statuses, [-1, 1])\n        batch_train_followers = tf.reshape(batch_train_followers, [-1, 1])\n        batch_train_friends_count = tf.reshape(batch_train_friends_count, [-1, 1])\n        batch_train_favourites_count = tf.reshape(batch_train_favourites_count, [-1, 1])\n        batch_train_enc_rest = tf.concat([batch_train_statuses, batch_train_followers, batch_train_friends_count, batch_train_favourites_count], axis=-1)\n\n        adj_train, adj_train_sparse = get_adj(batch_train_data)\n\n        # Reshape batch_out and convert to one-hot encoding\n        batch_train_out = tf.reshape(batch_train_out, [-1])\n        batch_train_out_one_hot = tf.one_hot(batch_train_out, depth=2)\n\n        with tf.device(device):\n            with tf.GradientTape() as tape:\n                predictions = model(\n                    [batch_train_data, adj_train_sparse, batch_train_statuses, batch_train_followers, batch_train_friends_count, batch_train_favourites_count, batch_train_enc_rest],\n                    training=True\n                )\n                loss_value = loss_fn(batch_train_out_one_hot, predictions)\n\n            # Compute gradients and update weights\n            gradients = tape.gradient(loss_value, model.trainable_variables)\n            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n            # Update metrics\n            total_loss_train += loss_value.numpy() * len(batch_train_data)\n            total_correct_train += tf.reduce_sum(\n                tf.cast(tf.equal(tf.argmax(predictions, axis=1), tf.cast(batch_train_out, tf.int64)), tf.float32)\n            ).numpy()\n            total_samples_train += len(batch_train_data)\n\n    # Calculate epoch-level training metrics\n    epoch_accuracy_train = total_correct_train / total_samples_train\n    epoch_loss_train = total_loss_train / total_samples_train\n    print(f\"Training Accuracy: {epoch_accuracy_train:.4f}\")\n    print(f\"Training Loss: {epoch_loss_train:.4f}\")\n\n    # Testing after each epoch\n    total_correct_test = 0\n    total_samples_test = 0\n    total_loss_test = 0\n\n    for i in tqdm(range(len(X_test_data_batches))):\n        batch_test_data = tf.convert_to_tensor(X_test_data_batches[i], dtype=tf.float32)\n        batch_test_out = tf.convert_to_tensor(X_test_out_batches[i], dtype=tf.int32)\n        batch_test_statuses_count = tf.convert_to_tensor(X_test_statuses_count_batches[i], dtype=tf.float32)\n        batch_test_followers_count = tf.convert_to_tensor(X_test_followers_count_batches[i], dtype=tf.float32)\n        batch_test_friends_count = tf.convert_to_tensor(X_test_friends_count_batches[i], dtype=tf.float32)\n        batch_test_favourites_count = tf.convert_to_tensor(X_test_favourites_count_batches[i], dtype=tf.float32)\n\n        # Combine features\n        batch_test_statuses_count = tf.reshape(batch_test_statuses_count, [-1, 1])\n        batch_test_followers_count = tf.reshape(batch_test_followers_count, [-1, 1])\n        batch_test_friends_count = tf.reshape(batch_test_friends_count, [-1, 1])\n        batch_test_favourites_count = tf.reshape(batch_test_favourites_count, [-1, 1])\n        test_enc_rest = tf.concat(\n            [batch_test_statuses_count, batch_test_followers_count, batch_test_friends_count, batch_test_favourites_count],\n            axis=-1\n        )\n\n        test_adj, test_adj_sparse = get_adj(batch_test_data)\n\n        # Reshape batch_test_out and convert to one-hot encoding\n        batch_test_out = tf.reshape(batch_test_out, [-1])\n        batch_out_one_hot = tf.one_hot(batch_test_out, depth=2)\n\n        with tf.device(device):\n            # Get predictions from the model\n            predictions = model(\n                [batch_test_data, test_adj_sparse, batch_test_statuses_count, batch_test_followers_count, batch_test_friends_count, batch_test_favourites_count, test_enc_rest],\n                training=False\n            )\n            loss_value = loss_fn(batch_out_one_hot, predictions)\n\n            # Update metrics\n            total_loss_test += loss_value.numpy() * len(batch_test_data)\n            total_correct_test += tf.reduce_sum(\n                tf.cast(tf.equal(tf.argmax(predictions, axis=1), tf.cast(batch_test_out, tf.int64)), tf.float32)\n            ).numpy()\n            total_samples_test += len(batch_test_data)\n\n    # Calculate test metrics\n    test_accuracy = total_correct_test / total_samples_test\n    test_loss = total_loss_test / total_samples_test\n    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n    print(f\"Test Loss: {test_loss:.4f}\")\n\n    # Validation after each epoch\n    total_correct_val = 0\n    total_samples_val = 0\n    total_loss_val = 0\n\n    for i in tqdm(range(len(X_val_data_batches))):\n        batch_val_data = tf.convert_to_tensor(X_val_data_batches[i], dtype=tf.float32)\n        batch_val_out = tf.convert_to_tensor(X_val_out_batches[i], dtype=tf.int32)\n        batch_val_statuses_count = tf.convert_to_tensor(X_val_statuses_count_batches[i], dtype=tf.float32)\n        batch_val_followers_count = tf.convert_to_tensor(X_val_followers_count_batches[i], dtype=tf.float32)\n        batch_val_friends_count = tf.convert_to_tensor(X_val_friends_count_batches[i], dtype=tf.float32)\n        batch_val_favourites_count = tf.convert_to_tensor(X_val_favourites_count_batches[i], dtype=tf.float32)\n\n        # Combine features\n        batch_val_statuses_count = tf.reshape(batch_val_statuses_count, [-1, 1])\n        batch_val_followers_count = tf.reshape(batch_val_followers_count, [-1, 1])\n        batch_val_friends_count = tf.reshape(batch_val_friends_count, [-1, 1])\n        batch_val_favourites_count = tf.reshape(batch_val_favourites_count, [-1, 1])\n        val_enc_rest = tf.concat(\n            [batch_val_statuses_count, batch_val_followers_count, batch_val_friends_count, batch_val_favourites_count],\n            axis=-1\n        )\n\n        val_adj, val_adj_sparse = get_adj(batch_val_data)\n\n        # Reshape batch_val_out and convert to one-hot encoding\n        batch_val_out = tf.reshape(batch_val_out, [-1])\n        batch_out_one_hot = tf.one_hot(batch_val_out, depth=2)\n\n        with tf.device(device):\n            # Get predictions from the model\n            predictions = model(\n                [batch_val_data, val_adj_sparse, batch_val_statuses_count, batch_val_followers_count, batch_val_friends_count, batch_val_favourites_count, val_enc_rest],\n                training=False\n            )\n            loss_value = loss_fn(batch_out_one_hot, predictions)\n\n            # Update metrics\n            total_loss_val += loss_value.numpy() * len(batch_val_data)\n            total_correct_val += tf.reduce_sum(\n                tf.cast(tf.equal(tf.argmax(predictions, axis=1), tf.cast(batch_val_out, tf.int64)), tf.float32)\n            ).numpy()\n            total_samples_val += len(batch_val_data)\n\n    # Calculate val metrics\n    val_accuracy = total_correct_val / total_samples_val\n    val_loss = total_loss_val / total_samples_val\n    print(f\"Val Accuracy: {val_accuracy:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f}\")\n\n    # Save model if both test and validation accuracies exceed 98%\n    if test_accuracy > 0.98 and val_accuracy > 0.98:\n        model.save(f'/kaggle/working/model_epoch_{epoch + 1}.h5')\n        print(f\"Model saved at epoch {epoch + 1} with Test Accuracy: {test_accuracy:.4f} and Val Accuracy: {val_accuracy:.4f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RhzfDQrTBznj","outputId":"0e13f139-c7b6-4e6d-c345-0eab3fc7ab08","trusted":true,"execution":{"iopub.status.busy":"2025-01-03T22:29:06.942182Z","iopub.execute_input":"2025-01-03T22:29:06.942586Z","iopub.status.idle":"2025-01-03T22:29:06.955833Z","shell.execute_reply.started":"2025-01-03T22:29:06.942566Z","shell.execute_reply":"2025-01-03T22:29:06.955059Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\nimport os\nimport re\n\n# Directory where saved models are stored\nmodel_dir = '/kaggle/input/saved-models/'  # Change this to your saved models directory\nmodel_pattern = r'model_epoch_(\\d+)\\.h5'\n\n# Initialize lists for storing classification reports\nclassification_reports = []\n\n# Load all saved models and iterate\nfor filename in os.listdir(model_dir):\n    if re.match(model_pattern, filename):\n        epoch_number = int(re.search(model_pattern, filename).group(1))\n        model_path = os.path.join(model_dir, filename)\n        model = tf.keras.models.load_model(model_path, custom_objects={'GraphSageConv': GraphSageConv})\n\n        # Evaluate model on test dataset\n        y_test_true = []\n        y_test_pred = []\n\n        for i in tqdm(range(len(X_test_data_batches))):\n            batch_test_data = tf.convert_to_tensor(X_test_data_batches[i], dtype=tf.float32)\n            batch_test_out = tf.convert_to_tensor(X_test_out_batches[i], dtype=tf.int32)\n            batch_test_statuses_count = tf.convert_to_tensor(X_test_statuses_count_batches[i], dtype=tf.float32)\n            batch_test_followers_count = tf.convert_to_tensor(X_test_followers_count_batches[i], dtype=tf.float32)\n            batch_test_friends_count = tf.convert_to_tensor(X_test_friends_count_batches[i], dtype=tf.float32)\n            batch_test_favourites_count = tf.convert_to_tensor(X_test_favourites_count_batches[i], dtype=tf.float32)\n\n            # Combine features\n            batch_test_statuses_count = tf.reshape(batch_test_statuses_count, [-1, 1])\n            batch_test_followers_count = tf.reshape(batch_test_followers_count, [-1, 1])\n            batch_test_friends_count = tf.reshape(batch_test_friends_count, [-1, 1])\n            batch_test_favourites_count = tf.reshape(batch_test_favourites_count, [-1, 1])\n            test_enc_rest = tf.concat(\n                [batch_test_statuses_count, batch_test_followers_count, batch_test_friends_count, batch_test_favourites_count],\n                axis=-1\n            )\n\n            test_adj, test_adj_sparse = get_adj(batch_test_data)\n\n            # Predictions\n            predictions = model(\n                [batch_test_data, test_adj_sparse, batch_test_statuses_count, batch_test_followers_count,\n                 batch_test_friends_count, batch_test_favourites_count, test_enc_rest], training=False)\n            y_test_true.extend(batch_test_out.numpy())\n            y_test_pred.extend(tf.argmax(predictions, axis=1).numpy())\n\n        # Generate test classification report\n        test_report = classification_report(y_test_true, y_test_pred, output_dict=True)\n        test_overall_accuracy = test_report['accuracy']\n        classification_reports.append({'Epoch': epoch_number, 'Phase': 'Test', 'Report': test_report, 'Accuracy': test_overall_accuracy})\n\n        # Evaluate model on validation dataset\n        y_val_true = []\n        y_val_pred = []\n\n        for i in tqdm(range(len(X_val_data_batches))):\n            batch_val_data = tf.convert_to_tensor(X_val_data_batches[i], dtype=tf.float32)\n            batch_val_out = tf.convert_to_tensor(X_val_out_batches[i], dtype=tf.int32)\n            batch_val_statuses_count = tf.convert_to_tensor(X_val_statuses_count_batches[i], dtype=tf.float32)\n            batch_val_followers_count = tf.convert_to_tensor(X_val_followers_count_batches[i], dtype=tf.float32)\n            batch_val_friends_count = tf.convert_to_tensor(X_val_friends_count_batches[i], dtype=tf.float32)\n            batch_val_favourites_count = tf.convert_to_tensor(X_val_favourites_count_batches[i], dtype=tf.float32)\n\n            # Combine features\n            batch_val_statuses_count = tf.reshape(batch_val_statuses_count, [-1, 1])\n            batch_val_followers_count = tf.reshape(batch_val_followers_count, [-1, 1])\n            batch_val_friends_count = tf.reshape(batch_val_friends_count, [-1, 1])\n            batch_val_favourites_count = tf.reshape(batch_val_favourites_count, [-1, 1])\n            val_enc_rest = tf.concat(\n                [batch_val_statuses_count, batch_val_followers_count, batch_val_friends_count, batch_val_favourites_count],\n                axis=-1\n            )\n\n            val_adj, val_adj_sparse = get_adj(batch_val_data)\n\n            # Predictions\n            predictions = model(\n                [batch_val_data, val_adj_sparse, batch_val_statuses_count, batch_val_followers_count,\n                 batch_val_friends_count, batch_val_favourites_count, val_enc_rest], training=False)\n            y_val_true.extend(batch_val_out.numpy())\n            y_val_pred.extend(tf.argmax(predictions, axis=1).numpy())\n\n        # Generate validation classification report\n        val_report = classification_report(y_val_true, y_val_pred, output_dict=True)\n        val_overall_accuracy = val_report['accuracy']\n        classification_reports.append({'Epoch': epoch_number, 'Phase': 'Validation', 'Report': val_report, 'Accuracy': val_overall_accuracy})\n\n# Save all classification reports to a CSV\nrows = []\nfor report in classification_reports:\n    epoch = report['Epoch']\n    phase = report['Phase']\n    overall_accuracy = report['Accuracy']\n    for class_name, metrics in report['Report'].items():\n        if class_name in ['accuracy', 'macro avg', 'weighted avg']:\n            continue\n        rows.append({\n            'Epoch': epoch,\n            'Phase': phase,\n            'Class': class_name,\n            'Precision': metrics['precision'],\n            'Recall': metrics['recall'],\n            'F1-Score': metrics['f1-score'],\n            'Support': metrics['support'],\n            'Overall Accuracy': overall_accuracy\n        })\n\n# Create a DataFrame and save as CSV\ndf = pd.DataFrame(rows)\ndf.to_csv('/kaggle/working/classification_reports.csv', index=False)\nprint(\"Classification reports saved to classification_reports.csv\")\n","metadata":{"id":"SgE0-bZt7qWU","trusted":true,"execution":{"iopub.status.busy":"2025-01-03T22:35:55.891942Z","iopub.execute_input":"2025-01-03T22:35:55.892483Z","iopub.status.idle":"2025-01-03T22:36:02.281492Z","shell.execute_reply.started":"2025-01-03T22:35:55.892452Z","shell.execute_reply":"2025-01-03T22:36:02.280715Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 13.90it/s]\n100%|██████████| 2/2 [00:00<00:00, 14.14it/s]\n100%|██████████| 3/3 [00:00<00:00, 14.23it/s]\n100%|██████████| 2/2 [00:00<00:00, 14.01it/s]\n100%|██████████| 3/3 [00:00<00:00, 24.94it/s]\n100%|██████████| 2/2 [00:00<00:00, 25.44it/s]\n100%|██████████| 3/3 [00:00<00:00, 14.37it/s]\n100%|██████████| 2/2 [00:00<00:00, 14.29it/s]\n100%|██████████| 3/3 [00:00<00:00, 24.72it/s]\n100%|██████████| 2/2 [00:00<00:00, 14.37it/s]\n100%|██████████| 3/3 [00:00<00:00, 14.34it/s]\n100%|██████████| 2/2 [00:00<00:00, 14.27it/s]\n100%|██████████| 3/3 [00:00<00:00, 24.27it/s]\n100%|██████████| 2/2 [00:00<00:00, 14.11it/s]\n100%|██████████| 3/3 [00:00<00:00, 22.91it/s]\n100%|██████████| 2/2 [00:00<00:00, 14.21it/s]\n100%|██████████| 3/3 [00:00<00:00, 24.96it/s]\n100%|██████████| 2/2 [00:00<00:00, 25.38it/s]","output_type":"stream"},{"name":"stdout","text":"Classification reports saved to classification_reports.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15}]}