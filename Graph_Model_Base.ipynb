{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlCkNe1Cpo0j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "class GraphConvolutionLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(GraphConvolutionLayer, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, adj_matrix, features):\n",
        "        support = torch.matmul(adj_matrix, features)\n",
        "        output = self.linear(support)\n",
        "        return output\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.gc1 = GraphConvolutionLayer(input_dim, hidden_dim)\n",
        "        self.gc2 = GraphConvolutionLayer(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, adj_matrix, features):\n",
        "        h1 = F.relu(self.gc1(adj_matrix, features))\n",
        "        output = self.gc2(adj_matrix, h1)\n",
        "        return output\n",
        "\n",
        "# Preprocess text and obtain BERT embeddings\n",
        "def get_bert_embeddings(texts):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    encoded_texts = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded_texts)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    return last_hidden_states\n",
        "\n",
        "# Constructing adjacency matrix (assuming undirected graph for simplicity)\n",
        "def construct_adjacency_matrix(features):\n",
        "    num_nodes = len(features)\n",
        "    adj_matrix = np.zeros((num_nodes, num_nodes))\n",
        "    for i, feature_i in enumerate(features):\n",
        "        for j, feature_j in enumerate(features):\n",
        "            if i != j:\n",
        "                # Compute cosine similarity between feature vectors\n",
        "                similarity = cosine_similarity(features[feature_i], features[feature_j])\n",
        "                adj_matrix[i, j] = similarity\n",
        "    return torch.tensor(adj_matrix, dtype=torch.float)\n",
        "\n",
        "# Example text data\n",
        "texts = [\"Example text 1\", \"Example text 2\", \"Example text 3\"]\n",
        "\n",
        "# Get BERT embeddings for text data\n",
        "bert_embeddings = get_bert_embeddings(texts)\n",
        "\n",
        "# Assuming input dimension is the sum of the BERT embedding size and the dimension of each feature vector\n",
        "input_dim = bert_embeddings.shape[1] + len(features['emoji'])\n",
        "hidden_dim = 16\n",
        "output_dim = 1  # Output dimension can be adjusted based on your task\n",
        "\n",
        "# Concatenate BERT embeddings with existing features\n",
        "concatenated_features = torch.cat((bert_embeddings.mean(dim=1), torch.tensor(list(features.values()), dtype=torch.float)), dim=1)\n",
        "\n",
        "# Construct adjacency matrix\n",
        "adj_matrix = construct_adjacency_matrix(features)\n",
        "\n",
        "# Initialize GCN model\n",
        "gcn_model = GCN(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Forward pass\n",
        "output = gcn_model(adj_matrix, concatenated_features)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "class GraphConvolutionLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(GraphConvolutionLayer, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, adj_matrix, features):\n",
        "        support = torch.matmul(adj_matrix, features)\n",
        "        output = self.linear(support)\n",
        "        return output\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.gc1 = GraphConvolutionLayer(input_dim, hidden_dim)\n",
        "        self.gc2 = GraphConvolutionLayer(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, adj_matrix, features):\n",
        "        h1 = F.relu(self.gc1(adj_matrix, features))\n",
        "        output = self.gc2(adj_matrix, h1)\n",
        "        return output\n",
        "\n",
        "# Constructing adjacency matrix (assuming undirected graph for simplicity)\n",
        "def construct_adjacency_matrix(features):\n",
        "    num_nodes = len(features)\n",
        "    adj_matrix = np.zeros((num_nodes, num_nodes))\n",
        "    for i, feature_i in enumerate(features):\n",
        "        for j, feature_j in enumerate(features):\n",
        "            if i != j:\n",
        "                # Compute cosine similarity between feature vectors\n",
        "                similarity = cosine_similarity(features[feature_i], features[feature_j])\n",
        "                adj_matrix[i, j] = similarity\n",
        "    return torch.tensor(adj_matrix, dtype=torch.float)\n",
        "\n",
        "# Example usage:\n",
        "features = {\n",
        "    'emoji': [0.5, 0.3, 0.1],\n",
        "    'hashtag': [0.2, 0.6, 0.4],\n",
        "    'sentiment': [0.8, -0.2, 0.5],\n",
        "    'semicolon': [1, 0, 0],\n",
        "    'tweet_length': [0.7, 0.4, 0.9],\n",
        "    'compound': [0.2, -0.5, 0.7],\n",
        "    'neutral': [0.1, 0.9, -0.3],\n",
        "    'url': [0.3, 0.2, 0.6],\n",
        "    'cosine_similarity': [0.9, 0.8, 0.7]\n",
        "}\n",
        "\n",
        "# Assuming input dimension is the same as the dimension of each feature vector\n",
        "input_dim = len(features['emoji'])\n",
        "hidden_dim = 16\n",
        "output_dim = 1  # Output dimension can be adjusted based on your task\n",
        "\n",
        "# Construct adjacency matrix\n",
        "adj_matrix = construct_adjacency_matrix(features)\n",
        "\n",
        "# Initialize GCN model\n",
        "gcn_model = GCN(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Forward pass\n",
        "output = gcn_model(adj_matrix, torch.tensor(list(features.values()), dtype=torch.float))\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "z-j3jdEiqUwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vector1, vector2):\n",
        "    dot_product = np.dot(vector1, vector2)\n",
        "    magnitude1 = np.linalg.norm(vector1)\n",
        "    magnitude2 = np.linalg.norm(vector2)\n",
        "    similarity = dot_product / (magnitude1 * magnitude2)\n",
        "    return similarity\n"
      ],
      "metadata": {
        "id": "c5OABnTyqFUp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}